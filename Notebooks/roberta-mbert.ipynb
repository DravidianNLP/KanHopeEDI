{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultichannelBERT.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMH6ljWe8nTJHFQtr2lsT8M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qwerty125-blip/KanHope/blob/main/Notebooks/roberta-mbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpI42BPrewYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de1cce0-9dde-4676-c8df-00fb1007c41e"
      },
      "source": [
        "!pip install transformers "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW9KDbKg7NQK"
      },
      "source": [
        "from transformers import BertModel\n",
        "import os\n",
        "import torch\n",
        "#from googletrans import Translator\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "SGkywNrC-JZy",
        "outputId": "945c426a-9fc2-4495-e418-a7c207735919"
      },
      "source": [
        "train = pd.read_csv('/content/multichannelhope.csv')\n",
        "#train['labels']=LabelEncoder().fit_transform(train['class'])  \n",
        "train=train.drop(columns=['Unnamed: 0'])\n",
        "train.head(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "      <th>translation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>YouTube subscriber at list lack kooda Ella yel...</td>\n",
              "      <td>1</td>\n",
              "      <td>YouTube Subscriber at Least Luck Reach Everywhere</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Thu nem yogyathe ge maha vishnu hesaru ettu en...</td>\n",
              "      <td>1</td>\n",
              "      <td>Thu Name is worthy of the name of Maha Vishnu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Super togare teppa</td>\n",
              "      <td>1</td>\n",
              "      <td>Super sharp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Super song</td>\n",
              "      <td>1</td>\n",
              "      <td>Super song</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lyrics Artha adoru Kay kaal aadsrappa</td>\n",
              "      <td>1</td>\n",
              "      <td>Lyrics mean adoru gay foot dance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Anna e game yavudu</td>\n",
              "      <td>1</td>\n",
              "      <td>Anna What's this game</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Nanu  weaks ninda huccha agbitidini boss song ...</td>\n",
              "      <td>0</td>\n",
              "      <td>I am crazy from Weeks to the Boss Song</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ನಮ್ಮ ಕನ್ನಡ ಇಂಡಸ್ಟ್ರಿ ಯಾಕೆ ನಮ್ಮ ಕಲಾವಿದರನ್ನ ಕೈ ಬ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Why our Kannada Industry ..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Ninyavaga hindi li nodidiyappa?? Swalpa link s...</td>\n",
              "      <td>1</td>\n",
              "      <td>When did you see Hindi li ?? Share a little li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ಚಂದನ್ ಶೆಟ್ಟಿ ಟ್ರೊಲ್ ವಿಡಿಯೋ ನೀವು ನೋಡಿ ನೆಗಡ್ಡೆ ಇ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Chandan Shetty Troll Video You See Negadre Idr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...                                        translation\n",
              "0  YouTube subscriber at list lack kooda Ella yel...  ...  YouTube Subscriber at Least Luck Reach Everywhere\n",
              "1  Thu nem yogyathe ge maha vishnu hesaru ettu en...  ...      Thu Name is worthy of the name of Maha Vishnu\n",
              "2                                 Super togare teppa  ...                                        Super sharp\n",
              "3                                         Super song  ...                                         Super song\n",
              "4              Lyrics Artha adoru Kay kaal aadsrappa  ...                   Lyrics mean adoru gay foot dance\n",
              "5                                 Anna e game yavudu  ...                              Anna What's this game\n",
              "6  Nanu  weaks ninda huccha agbitidini boss song ...  ...             I am crazy from Weeks to the Boss Song\n",
              "7  ನಮ್ಮ ಕನ್ನಡ ಇಂಡಸ್ಟ್ರಿ ಯಾಕೆ ನಮ್ಮ ಕಲಾವಿದರನ್ನ ಕೈ ಬ...  ...                        Why our Kannada Industry ..\n",
              "8  Ninyavaga hindi li nodidiyappa?? Swalpa link s...  ...  When did you see Hindi li ?? Share a little li...\n",
              "9  ಚಂದನ್ ಶೆಟ್ಟಿ ಟ್ರೊಲ್ ವಿಡಿಯೋ ನೀವು ನೋಡಿ ನೆಗಡ್ಡೆ ಇ...  ...  Chandan Shetty Troll Video You See Negadre Idr...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "eqhfnPgBBgjD",
        "outputId": "49a651cc-71b4-425b-8238-d9a386aad8bf"
      },
      "source": [
        "test = pd.read_csv('/content/multichannelhope_test.csv')\n",
        "#test['lables'] = LabelEncoder().fit_transform(test['class'])\n",
        "test['labels'] = test['lables']\n",
        "test = test.drop(columns=['Unnamed: 0', 'lables'])\n",
        "test.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>translation</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ban tik tok</td>\n",
              "      <td>Ban tik tok</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Avara desha dalli erodu aethu....nnHagadre Nam...</td>\n",
              "      <td>Avara desha dalli erodu aethu....nnHagadre Nam...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Game name  bro</td>\n",
              "      <td>Game name  bro</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ದಿಯಾ ಚಲನಚಿತ್ರ ತುಂಬಾನೇ ಚೆನ್ನಾಗಿದೆ. ಸ್ವಲ್ಪ ಪ್ರಚಾ...</td>\n",
              "      <td>ದಿಯಾ ಚಲನಚಿತ್ರ ತುಂಬಾನೇ ಚೆನ್ನಾಗಿದೆ. ಸ್ವಲ್ಪ ಪ್ರಚಾ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nija nija nam manegu bandidda</td>\n",
              "      <td>Nija nija nam manegu bandidda</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Unlike ಮಾಡಿದವರು ರಶ್ಮಿಕಾ ಅಭಿಮಾನಿಗಳು....</td>\n",
              "      <td>Unlike ಮಾಡಿದವರು ರಶ್ಮಿಕಾ ಅಭಿಮಾನಿಗಳು....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ಗುರು ಎಲ್ಲಿದ್ದೆ ಗುರು ಇಷ್ಟು ದಿನ rolling_on_the_f...</td>\n",
              "      <td>ಗುರು ಎಲ್ಲಿದ್ದೆ ಗುರು ಇಷ್ಟು ದಿನ rolling_on_the_f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Supr bro u r video</td>\n",
              "      <td>Supr bro u r video</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Nanu film nodade ne edict aagiddene</td>\n",
              "      <td>Nanu film nodade ne edict aagiddene</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Bere level aagi ede..appata kannada haadu...</td>\n",
              "      <td>Bere level aagi ede..appata kannada haadu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ... labels\n",
              "0                                        Ban tik tok  ...      1\n",
              "1  Avara desha dalli erodu aethu....nnHagadre Nam...  ...      1\n",
              "2                                     Game name  bro  ...      1\n",
              "3  ದಿಯಾ ಚಲನಚಿತ್ರ ತುಂಬಾನೇ ಚೆನ್ನಾಗಿದೆ. ಸ್ವಲ್ಪ ಪ್ರಚಾ...  ...      0\n",
              "4                      Nija nija nam manegu bandidda  ...      1\n",
              "5             Unlike ಮಾಡಿದವರು ರಶ್ಮಿಕಾ ಅಭಿಮಾನಿಗಳು....  ...      1\n",
              "6  ಗುರು ಎಲ್ಲಿದ್ದೆ ಗುರು ಇಷ್ಟು ದಿನ rolling_on_the_f...  ...      0\n",
              "7                                 Supr bro u r video  ...      1\n",
              "8                Nanu film nodade ne edict aagiddene  ...      1\n",
              "9       Bere level aagi ede..appata kannada haadu...  ...      0\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymL4e6OS14GF"
      },
      "source": [
        "class cfg:\n",
        "    debug = False\n",
        "    max_len = 64\n",
        "    batch_size = 64\n",
        "    bert = 'roberta-base'\n",
        "    mbert = 'bert-base-multilingual-cased'\n",
        "    lr = 2e-5\n",
        "    epochs = 10\n",
        "    save_path = '/content/Saved_Models'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VolrMQw383LL"
      },
      "source": [
        "def epoch_time(start_time,end_time):\n",
        "\telapsed_time = end_time - start_time\n",
        "\telapsed_mins = int(elapsed_time/60)\n",
        "\telapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "\treturn elapsed_mins,elapsed_secs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K0Wr2k3DJnT"
      },
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "class RFDataset(Dataset):\n",
        "  def __init__(self,text, translation, label,tokenizer1, tokenizer2, max_len):\n",
        "    self.text = text\n",
        "    self.translation = translation\n",
        "    self.label = label\n",
        "    self.tokenizer1 = tokenizer1\n",
        "    self.tokenizer2 = tokenizer2\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "  \n",
        "  def __getitem__(self,item):\n",
        "    text = str(self.text[item])\n",
        "    translation = str(self.translation[item])\n",
        "    label = self.label[item]\n",
        "\n",
        "    encoding1 = self.tokenizer1.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length = self.max_len,\n",
        "        return_token_type_ids = False,\n",
        "        padding = 'max_length',\n",
        "        return_attention_mask= True,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "    \n",
        "    encoding2 = self.tokenizer2.encode_plus(\n",
        "        translation,\n",
        "        add_special_tokens=True,\n",
        "        max_length = self.max_len,\n",
        "        return_token_type_ids = False,\n",
        "        padding = 'max_length',\n",
        "        return_attention_mask= True,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "    return {\n",
        "        'text' : text,\n",
        "        'translation': translation,\n",
        "        'input_ids1' : encoding1['input_ids'].flatten(),\n",
        "        'input_ids2' : encoding2['input_ids'].flatten(),\n",
        "        'attention_mask1' : encoding1['attention_mask'].flatten(),\n",
        "        'attention_mask2' : encoding2['attention_mask'].flatten(),\n",
        "        #'label' : torch.tensor(label,dtype=torch.float)\n",
        "        'label' : torch.tensor(label, dtype=torch.float)\n",
        "\n",
        "    }"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FJ_VL11EAtz"
      },
      "source": [
        "def create_data_loader(df, tokenizer1, tokenizer2, max_len, batch_size, shuffle=True):\n",
        "    \n",
        "    ds =  RFDataset(\n",
        "        text = df.sentence.to_numpy(),\n",
        "        translation = df.translation.to_numpy(),\n",
        "        label = df.labels.to_numpy(),\n",
        "        tokenizer1 = tokenizer1,\n",
        "        tokenizer2 = tokenizer2,\n",
        "        max_len = max_len \n",
        "    )\n",
        "    \n",
        "    return DataLoader(ds,\n",
        "                      batch_size = batch_size,\n",
        "                      shuffle=shuffle,\n",
        "                      num_workers = 2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnXKQfMp5nIf"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(cfg.mbert, return_dict=False)\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(cfg.bert, return_dict=False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhrIwdcCrw9X"
      },
      "source": [
        "train_data_loader = create_data_loader(train, tokenizer1,tokenizer2, cfg.max_len, cfg.batch_size, shuffle=False)\n",
        "test_data_loader = create_data_loader(test, tokenizer1, tokenizer2, cfg.max_len, cfg.batch_size, shuffle=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLLaa6FLGoxq"
      },
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "class LinearWeightedAvg(nn.Module):\n",
        "    def __init__(self, n_inputs):\n",
        "        super(LinearWeightedAvg, self).__init__()\n",
        "        self.weights = nn.ParameterList([nn.Parameter(torch.randn(1)) for i in range(n_inputs)])\n",
        "\n",
        "    def forward(self, input):\n",
        "        res = 0\n",
        "        for emb_idx, emb in enumerate(input):\n",
        "            res += emb * self.weights[emb_idx]\n",
        "        return res\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B45pYoyZEq1r"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "class MultichannelBERT(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(MultichannelBERT, self).__init__()\n",
        "    self.bert = AutoModel.from_pretrained('roberta-base', return_dict=False)\n",
        "    self.mbert = AutoModel.from_pretrained('bert-base-multilingual-cased', return_dict = False)\n",
        "    self.fc1 = nn.Linear(self.bert.config.hidden_size, 128)\n",
        "    self.fc2 = nn.Linear(self.mbert.config.hidden_size,128)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc3 = nn.Linear(256,n_classes)\n",
        "    self.drop = nn.Dropout(p=0.2)\n",
        "    #self.fc4 = nn.Linear(128, 1)\n",
        "\n",
        "  def forward(self,input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
        "\n",
        "    _, mbert_output = self.mbert(\n",
        "      input_ids = input_ids1,\n",
        "      attention_mask = attention_mask1\n",
        "    )\n",
        "\n",
        "    _, bert_output = self.bert(\n",
        "        input_ids = input_ids2,\n",
        "        attention_mask = attention_mask2\n",
        "    )\n",
        "    \n",
        "    bert_out = self.fc1(bert_output)\n",
        "    mbert_out = self.fc2(mbert_output)\n",
        "\n",
        "    #Merges both the outputs \n",
        "    merged = torch.cat((bert_out, mbert_out),1) \n",
        "    act = self.relu(merged)\n",
        "    out = self.drop(act)\n",
        "    return self.fc3(out) #self.fc4(bert_out), self.fc4(mbert_output)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L16PPfVGIy3p"
      },
      "source": [
        "model = MultichannelBERT(1)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKGx6gqlJOmX"
      },
      "source": [
        "def epoch_time(start_time,end_time):\n",
        "\telapsed_time = end_time - start_time\n",
        "\telapsed_mins = int(elapsed_time/60)\n",
        "\telapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "\treturn elapsed_mins,elapsed_secs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q-lJ1n02sJB"
      },
      "source": [
        "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for data in data_loader:\n",
        "        input_ids1 = data['input_ids1'].to(device)\n",
        "        attention_mask1 = data['attention_mask1'].to(device)\n",
        "        input_ids2 = data['input_ids2'].to(device)\n",
        "        attention_mask2 = data['attention_mask2'].to(device)\n",
        "        labels = data['label'].to(device)\n",
        "        labelsviewed = labels.view(labels.shape[0],1)\n",
        "         \n",
        "\n",
        "        outputs = model(\n",
        "            input_ids1=input_ids1,\n",
        "            attention_mask1=attention_mask1,\n",
        "            input_ids2 = input_ids2,\n",
        "            attention_mask2 =attention_mask2\n",
        "            )\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        preds = [0 if x < 0.5 else 1 for x in outputs]\n",
        "        preds = torch.tensor(preds).to(device)\n",
        "        loss = loss_fn(outputs,labelsviewed)\n",
        "        #loss = loss_fn(outputs, labels)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        torch.cuda.empty_cache()\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDCzEiAC6vxh"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for data in data_loader:\n",
        "        input_ids1 = data['input_ids1'].to(device)\n",
        "        attention_mask1 = data['attention_mask1'].to(device)\n",
        "        input_ids2 = data['input_ids2'].to(device)\n",
        "        attention_mask2 = data['attention_mask2'].to(device)\n",
        "        labels = data['label'].to(device)\n",
        "        labelsviewed = labels.view(labels.shape[0],1)\n",
        " \n",
        "        outputs = model(\n",
        "            input_ids1=input_ids1,\n",
        "            attention_mask1=attention_mask1,\n",
        "            input_ids2 = input_ids2,\n",
        "            attention_mask2 = attention_mask2\n",
        "        )\n",
        "\n",
        "        #_, preds = torch.max(outputs, dim=1)\n",
        "        preds = [0 if x < 0.5 else 1 for x in outputs]\n",
        "        preds = torch.tensor(preds).to(device)\n",
        "        loss = loss_fn(outputs, labelsviewed)\n",
        "        #loss = loss_fn(outputs, labels) \n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "        torch.cuda.empty_cache()\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtOEwXhz8gJ1"
      },
      "source": [
        "EPOCHS = 10\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "import time\n",
        "optimizer = AdamW(model.parameters(), lr=cfg.lr, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "#loss = nn.BCELoss().to(device)\n",
        "loss = nn.BCEWithLogitsLoss().to(device)\n",
        "#loss = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT-lYKRm7HWr",
        "outputId": "f2cf31d6-89dc-409d-eb91-9d3027000869"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "for epoch in range(EPOCHS):\n",
        " \n",
        " \n",
        "  start_time = time.time()\n",
        "  train_acc,train_loss = train_epoch(\n",
        "      model,\n",
        "      train_data_loader,\n",
        "      loss,\n",
        "      optimizer,\n",
        "      device,\n",
        "      scheduler,\n",
        "      len(train)\n",
        "  )\n",
        "   \n",
        "  \n",
        "  val_acc,val_loss = eval_model(\n",
        "      model,\n",
        "      test_data_loader,\n",
        "      loss,\n",
        "      device,\n",
        "      len(test)\n",
        "  )\n",
        "  \n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "  print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'Train Loss {train_loss} accuracy {train_acc}')\n",
        "  print(f'Val Loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 1m 28s\n",
            "Train Loss 0.46458914567684306 accuracy 0.7894926232457719\n",
            "Val Loss 0.5496874064207077 accuracy 0.7362459546925567\n",
            "\n",
            "Epoch: 02 | Epoch Time: 1m 28s\n",
            "Train Loss 0.3987639784470372 accuracy 0.8409499820079165\n",
            "Val Loss 0.5765828132629395 accuracy 0.7103559870550162\n",
            "\n",
            "Epoch: 03 | Epoch Time: 1m 28s\n",
            "Train Loss 0.3231043389131283 accuracy 0.8751349406261244\n",
            "Val Loss 0.5890236228704453 accuracy 0.7135922330097088\n",
            "\n",
            "Epoch: 04 | Epoch Time: 1m 28s\n",
            "Train Loss 0.26379170791171064 accuracy 0.9077006117308384\n",
            "Val Loss 0.6594173014163971 accuracy 0.7394822006472492\n",
            "\n",
            "Epoch: 05 | Epoch Time: 1m 28s\n",
            "Train Loss 0.2310358663571292 accuracy 0.9247930910399423\n",
            "Val Loss 0.7006612598896027 accuracy 0.7508090614886732\n",
            "\n",
            "Epoch: 06 | Epoch Time: 1m 28s\n",
            "Train Loss 0.1946725725870708 accuracy 0.9379273119827275\n",
            "Val Loss 0.7397001087665558 accuracy 0.7540453074433657\n",
            "\n",
            "Epoch: 07 | Epoch Time: 1m 28s\n",
            "Train Loss 0.17535368815578264 accuracy 0.9427851745232098\n",
            "Val Loss 0.8020942628383636 accuracy 0.755663430420712\n",
            "\n",
            "Epoch: 08 | Epoch Time: 1m 28s\n",
            "Train Loss 0.16149736397054004 accuracy 0.9458438287153652\n",
            "Val Loss 0.7861691832542419 accuracy 0.7605177993527509\n",
            "\n",
            "Epoch: 09 | Epoch Time: 1m 28s\n",
            "Train Loss 0.1581454565855621 accuracy 0.9474631162288593\n",
            "Val Loss 0.8169902205467224 accuracy 0.7524271844660194\n",
            "\n",
            "Epoch: 10 | Epoch Time: 1m 28s\n",
            "Train Loss 0.1465824517667636 accuracy 0.9541201871176681\n",
            "Val Loss 0.8389445900917053 accuracy 0.7621359223300971\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAX1-dK68wds"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  sentence = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "  with torch.no_grad():\n",
        "    for data in data_loader:\n",
        "      texts = data['text'] \n",
        "      input_ids1 = data['input_ids1'].to(device)\n",
        "      attention_mask1 = data['attention_mask1'].to(device)\n",
        "      input_ids2 = data['input_ids2'].to(device)\n",
        "      attention_mask2 = data['attention_mask2'].to(device)\n",
        "      labels = data[\"label\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids1=input_ids1,\n",
        "        attention_mask1=attention_mask1,\n",
        "        input_ids2 = input_ids2,\n",
        "        attention_mask2 = attention_mask2\n",
        "      )\n",
        "      #_, preds = torch.max(outputs, dim=1)\n",
        "      preds = [0 if x < 0.5 else 1 for x in outputs]\n",
        "      preds = torch.tensor(preds).to(device)      \n",
        "      sentence.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(outputs)\n",
        "      real_values.extend(labels)\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return sentence, predictions, prediction_probs, real_values"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNA3bNUToqv7"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(model,test_data_loader)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xqVCBLromlj",
        "outputId": "e70e35d7-b0d2-44a1-8f3d-3c784ec63da5"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix,f1_score\n",
        "class_name = ['Hope_speech','Non_hope_speech']\n",
        "print(classification_report(y_test, y_pred, target_names=class_name,zero_division=0, digits=3))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "    Hope_speech      0.690     0.614     0.650       228\n",
            "Non_hope_speech      0.788     0.838     0.812       390\n",
            "\n",
            "       accuracy                          0.756       618\n",
            "      macro avg      0.739     0.726     0.731       618\n",
            "   weighted avg      0.752     0.756     0.752       618\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAmqKZg6pdDy"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}